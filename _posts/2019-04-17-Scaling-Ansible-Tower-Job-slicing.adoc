= Scaling Ansible Tower - Job slicing

Customers managing 1000's of nodes

Tower gives us many benefits including HA & Scaling

== Tower HA

Recap Tower HA

== Calculating Capacity

Ansible uses forks to determine how many hosts we automate against in parallel. By default 5 forks are spawned but we can override this if for example we are automating against a large number of hosts. 

For Ansible Tower, we can calculate the number of available forks based on memory capacity or cpu capacity depending on the nature of the workload.

*memory capacity*

Protecting memory capacity is the default configuration and allows us to overcommit cpu while ensuring we don't run out of memory. To calculate the number of forks we remove 2048MB memory for the running of Tower services and then divide the remaining availble memory by *mem_per_fork* - which defaults to 100MB:

`(mem - 2048) / mem_per_fork`

For a Tower node with 4GB memory this results in a capacity of 20 forks:

`(4096 - 2048) / 100 = 20`

*cpu capacity*

For cpu capacity we multiply the number of cpus by *fork_per_cpu* - which defaults to 4.

`cpus * fork_per_cpu`

For a Tower node with 4 cores:

`4 * 4 = 16`

*Somewhere between memory and cpu*



If I were to run a job with more forks than the calculated capacity, then I run the risk of performance issues or running out of resources on our Tower nodes.

== Job Slicing

When a job is launched in Ansible Tower, it is executed on a single Tower node in the cluster. So effectively, ansible-playbook is run from a single node. If we are executing a playbook across a large number of hosts then we are not making use of all of the available capacity in our Ansible Tower cluster.

Job slicing is a new feature of Ansible Tower 3.4 and helps to address this issue. Job slicing allows us to distribute a job across multiple Tower nodes. 

Show example screenshots where slicing used.

Record video

