= Extending the reach of Ansible Tower

Some opening paragraph here.

Most customers don't have simple environments to manage. Customers will invariably have multiple datacenters, and within those datacenters they will most likely have isolated networks. 

But we can deploy Tower HA right? So why not just deploy a single Tower cluster with Tower nodes in different networks? *Link to install notes - geographically co-located* We can't do this because of RabbitMQ.

So how can we extend the reach of Ansible Tower into these isolated networks?

== Simple example environment

I'm going to use a really simple example here to explain what this might look like. In reality, you may have a lot more datacenters, a mix of public & private cloud and lot's of isolation between networks. In the following image, we have 3 datacenters (DC's). Ansible Tower is deployed in DC1. There is a DMZ in DC1 which is is segregated with a firewall. We also have DC2 and DC3. Again, seperated with firewalls.

image::https://cloudautomation.pharriso.co.uk/images/Ansible Tower Multi DC.png[]

In this scenario, in order for Tower to manage every node, we would need to open up firewall ports from Ansible Tower to every single managed node. This may not simply be SSH access either, we could be managing Windows hosts, network devices, cloud platforms and more. In the case of our DMZ, we are now opening up a lot of connections between our DMZ hosts and our core network which is not desireable.

We may also need to consider the affect of our Tower host managing a lot of devices in a remote datacenter. We might not have a great link between our sites so it may be useful to reduce the amount of traffic going from our Tower node to our remote sites.

So how do we manage these isolated devices more effectively?

== Two ways to extend the reach

=== Isolated nodes 

https://docs.ansible.com/ansible-tower/latest/html/administration/clustering.html[Isolated Nodes] are Tower nodes that aren't part of the core Tower cluster, but are capable of executing playbooks and ad-hoc jobs. These isolated nodes do not have the same constraints as the Tower cluster, meaning they don't need to be geographically co-located with the Tower nodes. This allows us to deploy them in remote datacenters. Another great use of isolated nodes is to deploy them into isolated networks such as DMZ's. Now we no longer need to open up firewall rules from Ansible Tower to every managed node. We can simply allow Tower to talk to the isolated nodes and they in turn manage the endpoints assigned to them. The following diagram shows isolated nodes deployed in each DC and DMZ. Note how we only need SSH (22/tcp) access between the Tower nodes and isolated nodes.

image::https://cloudautomation.pharriso.co.uk/images/Ansible Tower Multi DC with Isolated nodes.png[]

*How job execution works*

* A request to run a job (Adhoc job or playbook) is submitted to Ansible Tower via the UI or API.
* Job details are sent to the relevant isolated node using SSH.
* The isolated node executes the job.
* Tower polls the job status on the isolated node and reports the output.

*How do we use isolated nodes?*

Isolated nodes need to be assigned to objects within Tower. They can be assigned to either a job template, inventory or organisation. 

*What are the benefits of using isolated nodes?*

There are obvious benefits of using isolated nodes. They give us a way of targeting isolated networks and also give us a more efficient way of managing endpoints in remote datacenters - Tower only needs to send the job to the isolated node which in turn executes the job - keeping the execution local to the endpoints and reducing traffic over the network.

Another key benefit with isolated nodes is the fact that we are offloading the job execution away from the Tower nodes. This means that isolated nodes provide us with a means increasing the scale of our automation platform.

*Are there limitations with using isolated nodes?*

There may be times when we want a more dynamic way of determing which node Ansible should use to connect to an endpoint. With isolated nodes, we need to assign them to something like an inventory before job execution. A lot of the time this might be fine. Going back to our example, we would probably have separate inventories for DC1, DC1 DMZ, DC2 and DC3 so we could assign an isolated node to the respective inventory to ensure that jobs are executed correctly. But there may also be times when I want to execute a playbook across managed nodes that exist across all of those DC's. For example, push a patch to all Development servers across my entire estate. Of course, we could have multple job templates or use a workflow to achieve this but this might not always be practical. 

=== Jump host

https://docs.ansible.com/ansible-tower/latest/html/administration/tipsandtricks.html[Jump hosts] provide another option for extending the reach of Ansible Tower into isolated networks. They don't provide us with all of the benefits of isolated nodes, but they are slightly more dynamic in how we can associate them to managed endpoints. Jump hosts are simply used to proxy the ssh connection in situations where Tower does not have direct access to the endpoint. 



*image*

*How job execution works*

* A request to run a job (Adhoc job or playbook) is submitted to Ansible Tower via the UI or API.
* Tower executes the job

*how do we use jump hosts?*

To use a jump host, we need have an Ansible variable set. The variable would look something like this:

[source]
....
ansible_ssh_common_args: '-o ProxyCommand="ssh -W %h:%p -q <user>@<jump_server_name>"'
....

This can be assigned to groups of hosts for example using group_vars or host_vars.

*What are the benefits of using jump hosts?*

We can assign jump hosts to managed nodes or groups of managed nodes using Ansible variables. This gives us 



*Are there limitations with using jump hosts*

Jump hosts do not provide us with any additional capacity for automation as the Tower nodes are still responsible for executing the playbooks locally. The jump host is simply providing the proxied connection to the endpoint. In fact, the use of a jump host will add a performance penalty as we have an extra hop to make.

=== pros and cons



