<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Cloud and Automation]]></title><description><![CDATA[This blog focuses mainly on Red Hat products in the Cloud and Automation space. I Currently working as a Solution Architect for Red Hat in the UK.]]></description><link>http://cloudautomation.pharriso.co.uk</link><image><url>images/background.jpg</url><title>Cloud and Automation</title><link>http://cloudautomation.pharriso.co.uk</link></image><generator>RSS for Node</generator><lastBuildDate>Mon, 21 Jan 2019 19:32:57 GMT</lastBuildDate><atom:link href="http://cloudautomation.pharriso.co.uk/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[RHV Disaster Recovery - Part II]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In <a href="https://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html">part one</a> of this blog we looked at the different Disaster Recovery Solutions for RHV. In this part, we will look at the Active/Passive implementation in more detail, including how to set it up and run it. You&#8217;ll also find a recorded demo of the failover process.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A quick recap on what was discussed in part one of this blog. The Active/Passive architecture allows us to fail Virtual Machines over to a Disaster Recovery site in the event of a failure in the Primary site. The solution also provides the necessary orchestration to failback to the Primary site when appropriate. We leverage the power of Ansible to orchestrate the failover. The necessary Ansible roles are provided in a rpm called ovirt-ansible-disaster-recovery which is shipped as part of the RHV product.</p>
</div>
<div class="paragraph">
<p>From an underlying infrastructure perspective, we obviously need some hypervisors in our DR site to fail the workloads onto. The DR site also needs it&#8217;s own RHV Manager. We also rely on replicated storage between sites. The orchestration of the storage replication is not covered by the Ansible roles that are shipped with RHV. A storage administrator needs to setup the replication beforehand. They will also need to make the storage read/write in the secondary site at the point that DR is invoked.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s take a high-level view of the DR process. In advance of the disaster, one must generate the mapping file which maps entities from the Primary site to the DR site. For example, workloads in "Cluster1" in the Primary site should be failed over to "Cluster2" in my DR site. This mapping file is used during the failover and failback process.</p>
</div>
<div class="paragraph">
<p>In the event of a disaster, the storage must be made read/write in the DR site. The process for this will vary depending on the nature of the disaster and the type of storage. The "failover" Ansible playbook can then be run. This will mount the storage domains at the DR site. Virtual Machines are then registered and started. Any VM&#8217;s marked as Highly Available are started first.</p>
</div>
<div class="paragraph">
<p>Once the Primary site is brought back online, it must be prepared. This is part of the "clean" Ansible playbook. Any replicated storage domains need to be removed from the RHV Manager. A storage administrator will need to ensure replication is now taking place from the DR site to the Primary site.</p>
</div>
<div class="paragraph">
<p>When ready, the failback playbook can be run. This stops the VM&#8217;s in the DR site and removes the storage domains from the RHV Manager. The playbook then pauses and waits for the administrator to confirm that replication has been stopped and the storage is now read/write in the Primary site. Once confirmed, the playbook will import the storage domains at the Primary site, register the VM&#8217;s and start them up. The final step is to ensure the storage replication is switched so that the Primary site is once again replicating to the DR site.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo_environment">Demo Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I am using nested virtualisation for my lab. For the storage replication, I am using GlusterFS and geo-replication. This is asynchronous replication which is <strong>not</strong> recommended but it serves the purpose for this demo. Synchronous replication is recommended for replication between sites.</p>
</div>
<div class="paragraph">
<p>Each "site" has a RHV Manager. The RHV Manager is configured with a single cluster containing a singe RHV Host. I&#8217;ve got two small cirros VM&#8217;s which I will failover in this demo. One is marked as Highly Available to demonstrate the fact that HA VM&#8217;s are started before standard VM&#8217;s.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%20Lab.png" alt="RHV DR Active Passive Lab">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setup">Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>All of the files mentioned in this setup section are available in my github repo <a href="https://github.com/pharriso/rhv-dr">here</a></em>.</p>
</div>
<div class="paragraph">
<p><strong>Don&#8217;t worry. All of the below playbooks are fully documented in the Red Hat documentation for RHV DR.</strong></p>
</div>
<div class="sect2">
<h3 id="_generate_mapping">Generate Mapping</h3>
<div class="paragraph">
<p>First create a variable file containing the passwords for the RHV manager. I&#8217;m creating a file called passwords.yml here with these contents:</p>
</div>
<div class="listingblock">
<div class="title">passwords.yml</div>
<div class="content">
<pre class="highlight"><code>---
dr_sites_primary_password: Redhat123
dr_sites_secondary_password: Redhat123</code></pre>
</div>
</div>
<div class="paragraph">
<p>Encrypt the file with ansible-vault to avoid leaving passwords in plaintext.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-vault encrypt password.yml</pre>
</div>
</div>
<div class="paragraph">
<p>Now we can create the playbook that will generate the mapping file for us.</p>
</div>
<div class="listingblock">
<div class="title">generate_mappings.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Generate mapping
  hosts: localhost
  connection: local

  vars:
    site: https://rhvm-primary.example.com/ovirt-engine/api
    username: admin@internal
    password: "{{ dr_sites_primary_password }}"
    ca: /root/DR/primary_ca.pem
    var_file: disaster_recovery_vars.yml

  vars_files:
    - passwords.yml

  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now run the playbook to generate the mapping variable file. This will log into the RHV Manager at the Primary site and retrieve details on components such as storage, networking and clusters.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook generate_mappings.yml --tags generate_mapping --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>A file called disaster_recovery_vars.yml is generated. This needs to be edited to allow you to map components from the Primary site to the Disaster site. For example, the login details for the Disaster site:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>dr_sites_primary_url: https://rhvm-primary.example.com/ovirt-engine/api
dr_sites_primary_username: admin@internal
dr_sites_primary_ca_file: /root/DR/primary_ca.pem

# Please fill in the following properties for the secondary site:
dr_sites_secondary_url:  https://rhvm-secondary.example.com/ovirt-engine/api
dr_sites_secondary_username:  admin@internal
dr_sites_secondary_ca_file: /etc/pki/ovirt-engine/ca.pem</pre>
</div>
</div>
<div class="paragraph">
<p>And also the cluster mappings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Mapping for cluster
dr_cluster_mappings:
- primary_name: Primary
  # Fill the correlated cluster name in the secondary site for cluster 'Primary'
  secondary_name: Disaster</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_failover_failback_playbooks">Create Failover/Failback Playbooks</h3>
<div class="paragraph">
<p>Now we can create the playbook that we will use to initiate the failover from our Primary site to our DR site:</p>
</div>
<div class="listingblock">
<div class="title">failover.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failover RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: secondary
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>And the failback playbook to allow us to failback to our Primary site once it has been restored (The same playbook but with the source and target reversed):</p>
</div>
<div class="listingblock">
<div class="title">failback.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failback RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: primary
    dr_source_map: secondary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, the cleanup playbook. This is used to clean the Primary site ready for failback:</p>
</div>
<div class="listingblock">
<div class="title">clean_primary.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: clean RHV
  hosts: localhost
  connection: local
  vars:
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_failover">Failover</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To failover we need to ensure that the storage replication is stopped and is made read/write in the DR site. Once this is confirmed, we can run the Ansible playbook to failover.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failover.yaml --tags fail_over --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cleanup_and_failback">Cleanup and Failback</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the Primary site is brought back online we can begin the failback process. Storage now needs to be replicated from DR site back to Primary site. The primary site also needs to be cleaned to ensure storage domains are not imported.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook clean_primary.yml --tags clean_engine --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>When ready, initiate the failback. The playbook will pause and wait for you to confirm that the storage replication has been stopped and that storage domains are now read/write in the Primary site.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failback.yml --tags fail_back --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo">Demo</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following video demonstrates the failover process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/OC66G7_y8Vo?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</guid><pubDate>Wed, 16 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Infrastructure Migration Solution]]></title><description><![CDATA[<div class="paragraph">
<p>While VMware is undoubtedly a great virtualisation offering (I spent a number of users working with VMware tech) some customers are looking beyond traditional virtualisation. The growing adoption of containerisation is a great example of this and using RHV as a stepping stone to Container Native Virtualisation (CNV) is a good example use case.</p>
</div>
<div class="paragraph">
<p>Other customers simply want to save costs and are looking at alternative virtualisation offerings. Whatever the reason for moving from VMware, we need a method for migrating workloads to RHV.</p>
</div>
<div class="paragraph">
<p>This is what our Infrastructure Migration Solution (IMS) provides. A simple method for migrating workloads from VMware to RHV. We use a tool called CloudForms to drive the migration process (ManageIQ is the upstream project for CloudForms).</p>
</div>
<div class="paragraph">
<p>CloudForms is a manager of managers and can interact with various endpoints which are known as providers. A provider could be VMware vSphere, Red Hat Virtualisation, AWS, OpenStack etc. CloudForms is able to ingest information from these providers such as configuration details and performance metrics as well as orchestrate these providers.</p>
</div>
<div class="paragraph">
<p>Using the new CloudForms "migration" tooling we can create mappings between VMware infrastructure and Red Hat Virtualisation. These include clusters, storage and networking. We can then detect the VM&#8217;s currently running in VMware and using virt-v2v under the covers we can automate the migration of those VM&#8217;s across to RHV.</p>
</div>
<div class="paragraph">
<p>This short video shows a demo of the process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/NdjGuJaDSOU?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</guid><pubDate>Wed, 09 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[RHV Disaster Recovery - Part I]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of Red Hat Virtualisation (RHV) 4.1 back in 2017 we have seen the inclusion of Disaster Recovery (DR) solutions. The aim of these solutions is to allow customers to deploy their RHV infrastructure to span multiple sites and allow failover of virtual machines in the event of a disaster. This post will take a look at these Disaster Recovery solutions.</p>
</div>
<div class="paragraph">
<p>It is worth noting that these DR solutions are part of the core RHV product and are NOT part of a separate subscription offering. There is no per Virtual Machine (VM) cost or similar charge for protecting your workloads.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RHV 4.1 was released back in 2017 and we introduced an Active/Active implementation. With the release of RHV 4.2 in May 2018 we introduced additional disaster recovery capabilities. We can now deploy an Active/Passive architecture spanning two sites. Let&#8217;s take a look at the different implementation options.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_active">Active/Active</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Active architecture can span two sites. With this implementation we deploy a single RHV cluster with RHV hosts spanning both sites in a stretch cluster configuration. A single RHV Manager is responsible for managing both sites. The RHV Manager itself could be self hosted or standalone (self-hosted simply means the RHV Manager is a VM running on the hypervisors it is managing).</p>
</div>
<div class="paragraph">
<p>As both sites are active at the same time and VM&#8217;s can effectively run at either site at any given time we require synchronously replicated storage that is writeable at both sites at the same time.</p>
</div>
<div class="paragraph">
<p>In addition to the storage replication we also need a stretched network between sites. As we are deploying a single cluster across both sites we need all of the RHV hosts to be in the same Layer 2 network segment. Also VM networks need to be stretched between sites so that that VM&#8217;s can migrate or failover to the secondary site and maintain network connectivity.</p>
</div>
<div class="paragraph">
<p>VM to RHV Host affinity can be used to ensure VM&#8217;s are running in the Primary datacenter where possible and only failed over as part of a disaster. In the event of a disaster in the Primary site, any VM&#8217;s marked as "highly available" will automatically be restarted in the Disaster site without any administrator intervention.</p>
</div>
<div class="paragraph">
<p>The nice thing about the Active/Active setup is that we just rely on native RHV HA to fail VM&#8217;s between sites. It does rely on a storage array that can provide write access to both sites with replication. An example of this would be something like EMC VPLEX.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Active configuration looks like. This diagram depicts a the RHV manager as a self-hosted engine which is failed over along with the VM workloads.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%201.png" alt="RHV DR Active Active 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a failure at the Primary site, any VM&#8217;s marked as highly available will automatically restart on RHV hosts in the DR site. Also, as the RHV Manager is self-hosted it is also restarted at the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%202.png" alt="RHV DR Active Active 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_passive">Active/Passive</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Passive configuration can also be used across two sites. As the name suggests, only one site is active at any given time. In the event of a disaster, VM&#8217;s are failed over from the Primary site to the Disaster site. Once the Primary site is recovered we can then failback. Both failover and failback occur in an automated fashion using Ansible.</p>
</div>
<div class="paragraph">
<p>Unlike the Active/Active architecture, each site must maintain its own RHV Manager which manages the RHV hosts, storage and networks for that site.</p>
</div>
<div class="paragraph">
<p>From the storage perspective we require replicated storage between sites. However, the storage is only ever attached to one RHV site at a time so it does not need to be writeable at both sites simultaneously. The same VM networks need to be available in both sites so that VM&#8217;s that are failed over can be re-attached to the network.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Passive configuration looks like.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%201.png" alt="RHV DR Active Passive 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a disaster at the Primary site then we need to ensure that any replication of storage is stopped and that the storage is changed to read/write at the DR site and readonly at Primary site. An administrator can then initiate a failover of VM&#8217;s to the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%202.png" alt="RHV DR Active Passive 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of RHV 4.2 we now have two options for providing Disaster Recovery for our VM&#8217;s.</p>
</div>
<div class="paragraph">
<p>The Active/Active configuration allows us to easily fail workloads between sites using VM HA. However, it does rely on storage which is read/write at both sites at the same time as well as being synchronously replicated.</p>
</div>
<div class="paragraph">
<p>The Active/Passive solution only requires replicated storage in read/write at one site at a time. However, failover requires manual intervention to switch storage replication and also initiate the VM failover via Ansible.</p>
</div>
<div class="paragraph">
<p>In the next part of this post we will take a look at how to setup RHV DR in an Active/Passive configuration and how to perform failover and failback.</p>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</guid><pubDate>Tue, 08 Jan 2019 00:00:00 GMT</pubDate></item></channel></rss>