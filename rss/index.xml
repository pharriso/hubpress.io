<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Cloud and Automation]]></title><description><![CDATA[This blog focuses mainly on Red Hat products in the Cloud and Automation space. I am currently working as a Solution Architect for Red Hat in the UK. ]]></description><link>http://cloudautomation.pharriso.co.uk</link><image><url>images/background.jpg</url><title>Cloud and Automation</title><link>http://cloudautomation.pharriso.co.uk</link></image><generator>RSS for Node</generator><lastBuildDate>Mon, 29 Apr 2019 10:04:41 GMT</lastBuildDate><atom:link href="http://cloudautomation.pharriso.co.uk/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Scaling Ansible Tower - Job slicing]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Ansible Tower is a centralised platform for running and controlling your Ansible automation. It provides a number of key features for running Ansible in the enterprise.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>role-based access control</p>
</li>
<li>
<p>inventory management</p>
</li>
<li>
<p>credential management</p>
</li>
<li>
<p>auditing &amp; logging</p>
</li>
<li>
<p>clustering &amp; scale-out</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Job slicing is a new scale-out feature that was introduced in Ansible Tower 3.4 which allows us to run jobs that are distributed across our Ansible Tower cluster. Before we look at job slicing let&#8217;s quickly look at Tower clustering and job distribution in a cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ansible_tower_clustering">Ansible Tower clustering</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ansible Tower can be installed in a cluster to provide a highly available platform for running Ansible. Ansible Tower provides scale-out capabilities in that different jobs are scheduled across Tower nodes to ensure they are sharing the load. However, each individual job is run on a single Tower node only. If I run a job against 1000&#8217;s of servers then a single Tower node runs this job and it must have enough capacity to run this job in a timely fashion. Let&#8217;s look at how we calculate concurrency and capacity in Ansible Tower.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_calculating_capacity">Calculating Capacity</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ansible uses forks to determine how many hosts to automate against in parallel. By default 5 forks are spawned but we can override this if we are automating against a large number of hosts. For example, I may run a configuration management playbook every 1 hour to ensure my hosts are in a desired state. If I run that playbook against 1000&#8217;s of nodes with only 5 forks then this will take some time to complete.</p>
</div>
<div class="paragraph">
<p>For Ansible Tower, we can calculate the number of available forks based on memory capacity or cpu capacity depending on the nature of the workload.</p>
</div>
<div class="paragraph">
<p><strong>memory capacity</strong></p>
</div>
<div class="paragraph">
<p>Protecting memory capacity is the default configuration and allows us to overcommit cpu while ensuring we don&#8217;t run out of memory. To calculate the number of forks we remove 2048MB memory for the running of Tower services and then divide the remaining available memory by <strong>mem_per_fork</strong> - which defaults to 100MB:</p>
</div>
<div class="paragraph">
<p><code>(mem - 2048) / mem_per_fork</code></p>
</div>
<div class="paragraph">
<p>For a Tower node with 4GB memory this results in a capacity of around 20 forks:</p>
</div>
<div class="paragraph">
<p><code>(4096 - 2048) / 100 = 20</code></p>
</div>
<div class="paragraph">
<p><strong>cpu capacity</strong></p>
</div>
<div class="paragraph">
<p>For cpu capacity we multiply the number of cpus by <strong>fork_per_cpu</strong> - which defaults to 4.</p>
</div>
<div class="paragraph">
<p><code>cpus * fork_per_cpu</code></p>
</div>
<div class="paragraph">
<p>For a Tower node with 4 cores:</p>
</div>
<div class="paragraph">
<p><code>4 * 4 = 16</code></p>
</div>
<div class="paragraph">
<p>If I were to run a job with more forks than the calculated capacity, then I run the risk of performance issues or running out of resources on my Tower nodes.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_job_slicing">Job Slicing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As previously mentioned, when a job is launched in Ansible Tower, it is executed on a single Tower node in the cluster. So effectively, ansible-playbook is run from a single node. This means that a job is limited to the number of forks on a single Tower node. If we are executing a playbook across a large number of hosts then we are not making use of all of the available capacity in our Ansible Tower cluster.</p>
</div>
<div class="paragraph">
<p>Job slicing is a new feature of Ansible Tower 3.4 and helps to address this issue. Job slicing allows us to distribute a job across multiple Tower nodes. It does this by splitting the inventory into slices. A workflow is then created for us with multiple instances of our job being run on each slice of our inventory. For example, if I have 30 nodes in my inventory and I decide to create 3 slices then a workflow is created with three jobs - each being executed on 10 nodes in parallel. Obviously, I need 3 Tower nodes in this example so that my three job slices can be executed across the three hosts.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_job_slices">Using job slices</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To demonstrate job slicing, I have a three node Tower cluster. My inventory contains 49 hosts for me to automate against. Here is the available capacity in forks reported by my Tower hosts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/tower%20capacity.png" alt="tower capacity">
</div>
</div>
<div class="paragraph">
<p>My Tower hosts can each safely spawn up to 27 forks based on memory capacity. If we look at the following job template we can see that I have specified 30 forks - I run the risk of exhausting memory resources on my Tower node. Also note that I only have one job slice configured - this is the default when creating a new job template.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20template%201%20slice.png" alt="job template 1 slice">
</div>
</div>
<div class="paragraph">
<p>When I launch my job we can see that my playbook is being run on a single Tower node. As I have allocated more forks than I safely have capacity for I am over-allocated on capacity. My other two Tower nodes are just relaxing at this point with no work scheduled on them.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/1%20slice%20capacity.png" alt="1 slice capacity">
</div>
</div>
<div class="paragraph">
<p>Now I&#8217;ll re-configure my job template so that it utilises three slices.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20template%203%20slices.png" alt="job template 3 slices">
</div>
</div>
<div class="paragraph">
<p>This time when I launch my job template, a worklfow is automatically generated.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20slice%20workflow%20finished.png" alt="job slice workflow finished">
</div>
</div>
<div class="paragraph">
<p>If we look at one of the individual jobs within the workflow we can see that this is slice job 1 of 3. Also, note that this job is being run on a "slice" of my inventory - 16 hosts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20output.png" alt="job output">
</div>
</div>
<div class="paragraph">
<p>Finally, we can see that capacity is now being utilised across all three of my Tower nodes.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/3%20slice%20capacity.png" alt="3 slice capacity">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Job slices provide an additional method for scaling Ansible automation with Ansible Tower. By default, jobs are distributed across Tower nodes in a cluster but each individual job is only ever run from a single Tower node. Job slicing allows us to split a single job so that it is run across multiple nodes to provide additional scale-out capacity.</p>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/04/17/Scaling-Ansible-Tower-Job-slicing.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/04/17/Scaling-Ansible-Tower-Job-slicing.html</guid><pubDate>Wed, 17 Apr 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[RHV Disaster Recovery - Part II]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In <a href="https://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html">part one</a> of this blog we looked at the different Disaster Recovery Solutions for RHV. In this part, we will look at the Active/Passive implementation in more detail, including how to set it up and run it. You&#8217;ll also find a recorded demo of the failover process.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A quick recap on what was discussed in part one of this blog. The Active/Passive architecture allows us to fail Virtual Machines over to a Disaster Recovery site in the event of a failure in the Primary site. The solution also provides the necessary orchestration to failback to the Primary site when appropriate. We leverage the power of Ansible to orchestrate the failover. The necessary Ansible roles are provided in a rpm called ovirt-ansible-disaster-recovery which is shipped as part of the RHV product.</p>
</div>
<div class="paragraph">
<p>From an underlying infrastructure perspective, we obviously need some hypervisors in our DR site to fail the workloads onto. The DR site also needs it&#8217;s own RHV Manager. We also rely on replicated storage between sites. The orchestration of the storage replication is not covered by the Ansible roles that are shipped with RHV. A storage administrator needs to setup the replication beforehand. They will also need to make the storage read/write in the secondary site at the point that DR is invoked.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s take a high-level view of the DR process. In advance of the disaster, one must generate the mapping file which maps entities from the Primary site to the DR site. For example, workloads in "Cluster1" in the Primary site should be failed over to "Cluster2" in my DR site. This mapping file is used during the failover and failback process.</p>
</div>
<div class="paragraph">
<p>In the event of a disaster, the storage must be made read/write in the DR site. The process for this will vary depending on the nature of the disaster and the type of storage. The "failover" Ansible playbook can then be run. This will mount the storage domains at the DR site. Virtual Machines are then registered and started. Any VM&#8217;s marked as Highly Available are started first.</p>
</div>
<div class="paragraph">
<p>Once the Primary site is brought back online, it must be prepared. This is part of the "clean" Ansible playbook. Any replicated storage domains need to be removed from the RHV Manager. A storage administrator will need to ensure replication is now taking place from the DR site to the Primary site.</p>
</div>
<div class="paragraph">
<p>When ready, the failback playbook can be run. This stops the VM&#8217;s in the DR site and removes the storage domains from the RHV Manager. The playbook then pauses and waits for the administrator to confirm that replication has been stopped and the storage is now read/write in the Primary site. Once confirmed, the playbook will import the storage domains at the Primary site, register the VM&#8217;s and start them up. The final step is to ensure the storage replication is switched so that the Primary site is once again replicating to the DR site.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo_environment">Demo Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I am using nested virtualisation for my lab. For the storage replication, I am using GlusterFS and geo-replication. This is asynchronous replication which is <strong>not</strong> recommended but it serves the purpose for this demo. Synchronous replication is recommended for replication between sites.</p>
</div>
<div class="paragraph">
<p>Each "site" has a RHV Manager. The RHV Manager is configured with a single cluster containing a singe RHV Host. I&#8217;ve got two small cirros VM&#8217;s which I will failover in this demo. One is marked as Highly Available to demonstrate the fact that HA VM&#8217;s are started before standard VM&#8217;s.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%20Lab.png" alt="RHV DR Active Passive Lab">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setup">Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>All of the files mentioned in this setup section are available in my github repo <a href="https://github.com/pharriso/rhv-dr">here</a></em>.</p>
</div>
<div class="paragraph">
<p><strong>Don&#8217;t worry. All of the below playbooks are fully documented in the Red Hat documentation for RHV DR.</strong></p>
</div>
<div class="sect2">
<h3 id="_generate_mapping">Generate Mapping</h3>
<div class="paragraph">
<p>First create a variable file containing the passwords for the RHV manager. I&#8217;m creating a file called passwords.yml here with these contents:</p>
</div>
<div class="listingblock">
<div class="title">passwords.yml</div>
<div class="content">
<pre class="highlight"><code>---
dr_sites_primary_password: Redhat123
dr_sites_secondary_password: Redhat123</code></pre>
</div>
</div>
<div class="paragraph">
<p>Encrypt the file with ansible-vault to avoid leaving passwords in plaintext.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-vault encrypt password.yml</pre>
</div>
</div>
<div class="paragraph">
<p>Now we can create the playbook that will generate the mapping file for us.</p>
</div>
<div class="listingblock">
<div class="title">generate_mappings.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Generate mapping
  hosts: localhost
  connection: local

  vars:
    site: https://rhvm-primary.example.com/ovirt-engine/api
    username: admin@internal
    password: "{{ dr_sites_primary_password }}"
    ca: /root/DR/primary_ca.pem
    var_file: disaster_recovery_vars.yml

  vars_files:
    - passwords.yml

  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now run the playbook to generate the mapping variable file. This will log into the RHV Manager at the Primary site and retrieve details on components such as storage, networking and clusters.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook generate_mappings.yml --tags generate_mapping --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>A file called disaster_recovery_vars.yml is generated. This needs to be edited to allow you to map components from the Primary site to the Disaster site. For example, the login details for the Disaster site:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>dr_sites_primary_url: https://rhvm-primary.example.com/ovirt-engine/api
dr_sites_primary_username: admin@internal
dr_sites_primary_ca_file: /root/DR/primary_ca.pem

# Please fill in the following properties for the secondary site:
dr_sites_secondary_url:  https://rhvm-secondary.example.com/ovirt-engine/api
dr_sites_secondary_username:  admin@internal
dr_sites_secondary_ca_file: /etc/pki/ovirt-engine/ca.pem</pre>
</div>
</div>
<div class="paragraph">
<p>And also the cluster mappings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Mapping for cluster
dr_cluster_mappings:
- primary_name: Primary
  # Fill the correlated cluster name in the secondary site for cluster 'Primary'
  secondary_name: Disaster</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_failover_failback_playbooks">Create Failover/Failback Playbooks</h3>
<div class="paragraph">
<p>Now we can create the playbook that we will use to initiate the failover from our Primary site to our DR site:</p>
</div>
<div class="listingblock">
<div class="title">failover.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failover RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: secondary
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>And the failback playbook to allow us to failback to our Primary site once it has been restored (The same playbook but with the source and target reversed):</p>
</div>
<div class="listingblock">
<div class="title">failback.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failback RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: primary
    dr_source_map: secondary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, the cleanup playbook. This is used to clean the Primary site ready for failback:</p>
</div>
<div class="listingblock">
<div class="title">clean_primary.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: clean RHV
  hosts: localhost
  connection: local
  vars:
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_failover">Failover</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To failover we need to ensure that the storage replication is stopped and is made read/write in the DR site. Once this is confirmed, we can run the Ansible playbook to failover.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failover.yaml --tags fail_over --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cleanup_and_failback">Cleanup and Failback</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the Primary site is brought back online we can begin the failback process. Storage now needs to be replicated from DR site back to Primary site. The primary site also needs to be cleaned to ensure storage domains are not imported.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook clean_primary.yml --tags clean_engine --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>When ready, initiate the failback. The playbook will pause and wait for you to confirm that the storage replication has been stopped and that storage domains are now read/write in the Primary site.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failback.yml --tags fail_back --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo">Demo</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following video demonstrates the failover process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/OC66G7_y8Vo?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</guid><pubDate>Wed, 16 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Infrastructure Migration Solution]]></title><description><![CDATA[<div class="paragraph">
<p>While VMware is undoubtedly a great virtualisation offering (I spent a number of users working with VMware tech) some customers are looking beyond traditional virtualisation. The growing adoption of containerisation is a great example of this and using RHV as a stepping stone to Container Native Virtualisation (CNV) is a good example use case.</p>
</div>
<div class="paragraph">
<p>Other customers simply want to save costs and are looking at alternative virtualisation offerings. Whatever the reason for moving from VMware, we need a method for migrating workloads to RHV.</p>
</div>
<div class="paragraph">
<p>This is what our Infrastructure Migration Solution (IMS) provides. A simple method for migrating workloads from VMware to RHV. We use a tool called CloudForms to drive the migration process (ManageIQ is the upstream project for CloudForms).</p>
</div>
<div class="paragraph">
<p>CloudForms is a manager of managers and can interact with various endpoints which are known as providers. A provider could be VMware vSphere, Red Hat Virtualisation, AWS, OpenStack etc. CloudForms is able to ingest information from these providers such as configuration details and performance metrics as well as orchestrate these providers.</p>
</div>
<div class="paragraph">
<p>Using the new CloudForms "migration" tooling we can create mappings between VMware infrastructure and Red Hat Virtualisation. These include clusters, storage and networking. We can then detect the VM&#8217;s currently running in VMware and using virt-v2v under the covers we can automate the migration of those VM&#8217;s across to RHV.</p>
</div>
<div class="paragraph">
<p>This short video shows a demo of the process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/NdjGuJaDSOU?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</guid><pubDate>Wed, 09 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[RHV Disaster Recovery - Part I]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of Red Hat Virtualisation (RHV) 4.1 back in 2017 we have seen the inclusion of Disaster Recovery (DR) solutions. The aim of these solutions is to allow customers to deploy their RHV infrastructure to span multiple sites and allow failover of virtual machines in the event of a disaster. This post will take a look at these Disaster Recovery solutions.</p>
</div>
<div class="paragraph">
<p>It is worth noting that these DR solutions are part of the core RHV product and are NOT part of a separate subscription offering. There is no per Virtual Machine (VM) cost or similar charge for protecting your workloads.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RHV 4.1 was released back in 2017 and we introduced an Active/Active implementation. With the release of RHV 4.2 in May 2018 we introduced additional disaster recovery capabilities. We can now deploy an Active/Passive architecture spanning two sites. Let&#8217;s take a look at the different implementation options.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_active">Active/Active</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Active architecture can span two sites. With this implementation we deploy a single RHV cluster with RHV hosts spanning both sites in a stretch cluster configuration. A single RHV Manager is responsible for managing both sites. The RHV Manager itself could be self hosted or standalone (self-hosted simply means the RHV Manager is a VM running on the hypervisors it is managing).</p>
</div>
<div class="paragraph">
<p>As both sites are active at the same time and VM&#8217;s can effectively run at either site at any given time we require synchronously replicated storage that is writeable at both sites at the same time.</p>
</div>
<div class="paragraph">
<p>In addition to the storage replication we also need a stretched network between sites. As we are deploying a single cluster across both sites we need all of the RHV hosts to be in the same Layer 2 network segment. Also VM networks need to be stretched between sites so that that VM&#8217;s can migrate or failover to the secondary site and maintain network connectivity.</p>
</div>
<div class="paragraph">
<p>VM to RHV Host affinity can be used to ensure VM&#8217;s are running in the Primary datacenter where possible and only failed over as part of a disaster. In the event of a disaster in the Primary site, any VM&#8217;s marked as "highly available" will automatically be restarted in the Disaster site without any administrator intervention.</p>
</div>
<div class="paragraph">
<p>The nice thing about the Active/Active setup is that we just rely on native RHV HA to fail VM&#8217;s between sites. It does rely on a storage array that can provide write access to both sites with replication. An example of this would be something like EMC VPLEX.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Active configuration looks like. This diagram depicts a the RHV manager as a self-hosted engine which is failed over along with the VM workloads.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%201.png" alt="RHV DR Active Active 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a failure at the Primary site, any VM&#8217;s marked as highly available will automatically restart on RHV hosts in the DR site. Also, as the RHV Manager is self-hosted it is also restarted at the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%202.png" alt="RHV DR Active Active 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_passive">Active/Passive</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Passive configuration can also be used across two sites. As the name suggests, only one site is active at any given time. In the event of a disaster, VM&#8217;s are failed over from the Primary site to the Disaster site. Once the Primary site is recovered we can then failback. Both failover and failback occur in an automated fashion using Ansible.</p>
</div>
<div class="paragraph">
<p>Unlike the Active/Active architecture, each site must maintain its own RHV Manager which manages the RHV hosts, storage and networks for that site.</p>
</div>
<div class="paragraph">
<p>From the storage perspective we require replicated storage between sites. However, the storage is only ever attached to one RHV site at a time so it does not need to be writeable at both sites simultaneously. The same VM networks need to be available in both sites so that VM&#8217;s that are failed over can be re-attached to the network.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Passive configuration looks like.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%201.png" alt="RHV DR Active Passive 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a disaster at the Primary site then we need to ensure that any replication of storage is stopped and that the storage is changed to read/write at the DR site and readonly at Primary site. An administrator can then initiate a failover of VM&#8217;s to the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%202.png" alt="RHV DR Active Passive 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of RHV 4.2 we now have two options for providing Disaster Recovery for our VM&#8217;s.</p>
</div>
<div class="paragraph">
<p>The Active/Active configuration allows us to easily fail workloads between sites using VM HA. However, it does rely on storage which is read/write at both sites at the same time as well as being synchronously replicated.</p>
</div>
<div class="paragraph">
<p>The Active/Passive solution only requires replicated storage in read/write at one site at a time. However, failover requires manual intervention to switch storage replication and also initiate the VM failover via Ansible.</p>
</div>
<div class="paragraph">
<p>In the next part of this post we will take a look at how to setup RHV DR in an Active/Passive configuration and how to perform failover and failback.</p>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</guid><pubDate>Tue, 08 Jan 2019 00:00:00 GMT</pubDate></item></channel></rss>