<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Cloud and Automation]]></title><description><![CDATA[This blog focuses mainly on Red Hat products in the Cloud and Automation space. I am currently working as a Solution Architect for Red Hat in the UK. ]]></description><link>http://cloudautomation.pharriso.co.uk</link><image><url>images/background.jpg</url><title>Cloud and Automation</title><link>http://cloudautomation.pharriso.co.uk</link></image><generator>RSS for Node</generator><lastBuildDate>Tue, 06 Aug 2019 13:25:18 GMT</lastBuildDate><atom:link href="http://cloudautomation.pharriso.co.uk/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Call Ansible Tower from ServiceNow]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In this post we will look at how we can call Ansible Tower from ServiceNow as part of a ServiceNow Catalog Request. In this example, I have a job template in Ansible Tower that will manage the membership of a node within an F5 loadbalancer pool. The job expects the user to input the name of the node that should be managed and the state it should be in - either enabled or forced_offline. This input then needs to be passed into Ansible Tower as extra variables so that the automation job can run succesfully.</p>
</div>
<div class="paragraph">
<p>This integration allows us to use Tower for what it does best - provide the enterprise platform for executing our Ansible playbooks. We can leave Tower to manage credentials, provide RBAC and capture the audit history. At the same time we can leverage existing process in ServiceNow like approval processes and general change management workflows.</p>
</div>
<div class="paragraph">
<p><strong>NOTE</strong> I am not a ServiceNow developer and this is definitely not a tutorial in how best to use ServiceNow. This is something that I had to work out for a demo around Ansible Tower.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_ansible_tower_job_template">Ansible Tower Job Template</h3>
<div class="paragraph">
<p>I already have my <a href="https://docs.ansible.com/ansible-tower/latest/html/userguide/job_templates.html">job template</a> defined in Ansible Tower. Make a note of the job ID which is displayed in the URL when you click on the Job Template in Tower. For example the job ID is 10 in this URL - <a href="https://tower.example.com/#/templates/job_template/10" class="bare">https://tower.example.com/#/templates/job_template/10</a></p>
</div>
<div class="paragraph">
<p>To allow extra variables to be passed into the Ansibe job execution, I need to enable <strong>Prompt on launch</strong> for <strong>extra variables</strong> as per the screenshot below.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_tower_job_template.png" alt="snow tower job template">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_servicenow_outbound_rest_message">ServiceNow Outbound REST Message</h3>
<div class="paragraph">
<p>The first thing we need to configure in ServiceNow, is an Outbound REST message. This defines how it will connect to the Ansible Tower API to launch a job. This includes the API call, credentials and any extra variables we might want to pass from ServiceNow to Ansible Tower.</p>
</div>
<div class="paragraph">
<p>In ServiceNow, navigate to <strong>System Web Services &#8594; Outbound &#8594; REST Message</strong> and click <strong>New</strong>.</p>
</div>
<div class="paragraph">
<p>Enter a name and the URL for the REST endpoint. This should be your Ansible Tower URL with the job ID that you want to launch.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_rest_name.png" alt="snow rest name">
</div>
</div>
<div class="paragraph">
<p>Set the <strong>Authentication type</strong> to <strong>basic</strong> and then click on the magnifying glass next to <strong>Basic Auth profile</strong>. Click <strong>New</strong> and enter a name for credential &amp; the username and password to authenticate to Tower.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_cred.png" alt="snow cred">
</div>
</div>
<div class="paragraph">
<p>Next set the Content-Type by selecting the <strong>HTTP Request</strong> tab and then adding a new <strong>HTTP Header</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_endpoint_http_request.png" alt="snow endpoint http request">
</div>
</div>
<div class="paragraph">
<p>Click submit and then go back into your REST Message. As mentioned earlier, I want to pass variables from ServiceNow to Ansible Tower so I can use them in my Ansible automation. To do this, I need to add some content to the HTTP Post. Under <strong>HTTP Methods</strong> click <strong>New</strong> and add the details for our POST Request.  The <strong>HTTP Method</strong> needs to be <strong>POST</strong> and the <strong>Endpoint</strong> needs to be the url for the job we are launching.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_post.png" alt="snow post">
</div>
</div>
<div class="paragraph">
<p>Now under the <strong>HTTP Request</strong> tab, add a new <strong>HTTP Query Parameter</strong> with a <strong>Name</strong> of <strong>Content</strong> and a <strong>Value</strong> which contains the variable data I will pass to Tower. In my example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>{"extra_vars": { "member_name": "${f5_member}", "snow_request": "${snow_request}", "member_state": "${f5_member_state}" } }</pre>
</div>
</div>
<div class="paragraph">
<p>Note the <strong>${snow_request}</strong> variable here. We will come back to that later.</p>
</div>
<div class="paragraph">
<p>The <strong>HTTP Query Parameter</strong> should look as follows.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_extra_vars.png" alt="snow extra vars">
</div>
</div>
<div class="paragraph">
<p>Now we have defined how ServiceNow will make an API call to Ansible Tower. We need to create a workflow to define how this API call will be triggered.</p>
</div>
</div>
<div class="sect2">
<h3 id="_servicenow_workflow">ServiceNow Workflow</h3>
<div class="paragraph">
<p>In ServiceNow, navigate to <strong>Workflow &#8594; Editor</strong>. Select <strong>New Workflow</strong>. Name the workflow and set the table to <strong>Requested Item [sc_req_item]</strong>. Press submit and you will be taken into the workflow design canvas.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_workflow_name.png" alt="snow workflow name">
</div>
</div>
<div class="paragraph">
<p>Delete the connector between Begin and End. Then Select the <strong>Core</strong> tab in the right hand pane. Under <strong>Utilities</strong>, drag the <strong>Run Script</strong> object onto the workflow canvas. Name the script and then paste the script contents in.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>try {
 var r = new sn_ws.RESTMessageV2('Tower Job Launch', 'F5 Member Job'); <i class="conum" data-value="1"></i><b>(1)</b>
 r.setStringParameterNoEscape('f5_member', current.variables.f5_member);  <i class="conum" data-value="2"></i><b>(2)</b>
 r.setStringParameterNoEscape('f5_member_state', current.variables.f5_member_state);
 r.setStringParameterNoEscape('snow_request', current.request.number);  <i class="conum" data-value="3"></i><b>(3)</b>

 var response = r.execute();
 var responseBody = response.getBody();
 var httpStatus = response.getStatusCode();
}
catch(ex) {
 var message = ex.message;
}</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The name of the Outbound REST Message and HTTP Method name.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Example variable that will be passed from Catalog request.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>This is a special var we are determining from the ServiceNow catalog request. We will pass the request number that is created by the user as a variable called <strong>snow_request</strong></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now let&#8217;s drag an approval task onto our workflow. Again, in the <strong>Core</strong> tab, drag the <strong>Approval User</strong> or <strong>Approval Group</strong> item across. Name the approval step and select the user or group that needs to approve.</p>
</div>
<div class="paragraph">
<p>Finally, let&#8217;s connect our workflow up. Drag from small orange box inside <strong>Begin</strong> to your <strong>Approval</strong> task. Then drag from <strong>Approved</strong> to our <strong>Run Script</strong>. Then drag from <strong>Rejected</strong> to <strong>End</strong>. Lastly, drag from the <strong>Script</strong> to <strong>End</strong>. The flow should look as follows.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_workflow_image.png" alt="snow workflow image">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_servicenow_service_catalog">ServiceNow Service Catalog</h3>
<div class="paragraph">
<p>Now that we have defined out outbound REST call and our workflow, we can add a Service Catalog item to expose this to end users. In ServiceNow, navigate to <strong>Service Catalog &#8594; Catalog Definitions &#8594; Maintain Items</strong>. Click on <strong>New</strong> and enter a name for the Service Catalog item. Select the <strong>Catalogs</strong> you want the item to appear in. I have just selected <strong>Service Catalog</strong> for this example. Then in the <strong>Process Engine</strong> tab, search for your workflow.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_service_request_workflow.png" alt="snow service request workflow">
</div>
</div>
<div class="paragraph">
<p>Now we can define the information we want to prompt the user for which will be passed to Ansible Tower as variables. Within your Catalog Item, navigate to the <strong>Variables</strong> tab at the bottom of the screen. Click <strong>New</strong> to define a new variable.</p>
</div>
<div class="paragraph">
<p>Select the question <strong>Type</strong> e.g. single line text or multi-choice. Then enter the question you want the user to see in the Catalog item in the <strong>Question</strong> field. In the <strong>Name</strong> field, enter the name of the variable that you want to pass to Tower.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_sc_var.png" alt="snow sc var">
</div>
</div>
<div class="paragraph">
<p>For multi-choice variables, you need to add <strong>Question Choices</strong> at the bottom of this screen. The <strong>Text</strong> being what you want the user to see on the form and the <strong>Value</strong> being the value of the variable that will be passed to Tower.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/snow_sc_question_choices.png" alt="snow sc question choices">
</div>
</div>
</div>
<div class="sect2">
<h3 id="_executing_the_workflow">Executing the workflow</h3>
<div class="paragraph">
<p>So what should happen when we order this Catalog item?</p>
</div>
<div class="ulist">
<ul>
<li>
<p>User orders Catalog Item. User is asked to input the name of the host they want to manage in the F5 Loadbalancer and set the state of the host (multi-choice)</p>
</li>
<li>
<p>The request is raised and will await approval</p>
</li>
<li>
<p>Once the request is approved ServiceNow will launch the playbook via the Tower API. The answers that the user provided to the Catalog item will be passed as variables to the Ansible Tower playbook. We will also pass the ServiceNow request number.</p>
</li>
<li>
<p>Ansible will manage the host in the F5 loadbalancer pool as requested by the user.</p>
</li>
<li>
<p>Ansible will update the ticket in ServiceNow and close the ticket.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_acknowledgements">Acknowledgements</h3>
<div class="paragraph">
<p>There were two excellent guides I used for this. I just wanted to piece the best bits from both guides together:</p>
</div>
<div class="paragraph">
<p><a href="https://liveaverage.com/blog/ansible-tower-and-servicenow-integration-in-10-minutes/" class="bare">https://liveaverage.com/blog/ansible-tower-and-servicenow-integration-in-10-minutes/</a></p>
</div>
<div class="paragraph">
<p><a href="https://github.com/eanylin/ansible-lab/tree/master/servicenow_demo/" class="bare">https://github.com/eanylin/ansible-lab/tree/master/servicenow_demo/</a></p>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/08/05/Call-Ansible-Tower-from-Service-Now.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/08/05/Call-Ansible-Tower-from-Service-Now.html</guid><pubDate>Mon, 05 Aug 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Scaling Ansible Tower - Job slicing]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Ansible Tower is a centralised platform for running and controlling your Ansible automation. It provides a number of key features for running Ansible in the enterprise.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Role-Based Access Control</p>
</li>
<li>
<p>Inventory Management</p>
</li>
<li>
<p>Credential Management</p>
</li>
<li>
<p>Auditing &amp; Logging</p>
</li>
<li>
<p>Clustering &amp; Scale-Out</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Job slicing is a new scale-out feature that was introduced in Ansible Tower 3.4 which allows us to run jobs that are distributed across our Ansible Tower cluster. Before we look at job slicing let&#8217;s quickly look at  clustering and job distribution within a cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_ansible_tower_clustering">Ansible Tower clustering</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ansible Tower can be installed in a cluster to provide a highly available platform for running Ansible. A clustered install provides scale-out capabilities in that different jobs are scheduled across Tower nodes to ensure they are sharing the load. However, each individual job is run on a single Tower node only. If I run a job against 1000&#8217;s of servers then a single Tower node runs this job and it must have enough capacity to run this job in a timely fashion. Let&#8217;s look at how we calculate concurrency and capacity in Ansible Tower.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_calculating_capacity">Calculating Capacity</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ansible uses forks to determine how many hosts to automate against in parallel. By default 5 forks are spawned but we can override this if we are automating against a large number of hosts. For example, I may run a configuration management playbook every 1 hour to ensure my hosts are in a desired state. If I run that playbook against 1000&#8217;s of nodes with only 5 forks then this will take some time to complete.</p>
</div>
<div class="paragraph">
<p>We can calculate the number of available forks based on memory capacity or cpu capacity depending on the nature of the workload.</p>
</div>
<div class="paragraph">
<p><strong>memory capacity</strong></p>
</div>
<div class="paragraph">
<p>Protecting memory capacity is the default configuration and allows us to overcommit cpu while ensuring we don&#8217;t run out of memory. To calculate the number of forks we remove 2048MB memory for the running of Tower services and then divide the remaining available memory by <strong>mem_per_fork</strong> - which defaults to 100MB:</p>
</div>
<div class="paragraph">
<p><code>(mem - 2048) / mem_per_fork</code></p>
</div>
<div class="paragraph">
<p>For a Tower node with 4GB memory this results in a capacity of around 20 forks:</p>
</div>
<div class="paragraph">
<p><code>(4096 - 2048) / 100 = 20</code></p>
</div>
<div class="paragraph">
<p><strong>cpu capacity</strong></p>
</div>
<div class="paragraph">
<p>For cpu capacity we multiply the number of cpus by <strong>fork_per_cpu</strong> - which defaults to 4.</p>
</div>
<div class="paragraph">
<p><code>cpus * fork_per_cpu</code></p>
</div>
<div class="paragraph">
<p>For a Tower node with 4 cores:</p>
</div>
<div class="paragraph">
<p><code>4 * 4 = 16</code></p>
</div>
<div class="paragraph">
<p>If I were to run a job with more forks than the calculated capacity, then I run the risk of performance issues or running out of resources on my Tower nodes.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_job_slicing">Job Slicing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As previously mentioned, when a job is launched in Ansible Tower, it is executed on a single node in the cluster. So effectively, ansible-playbook is run from a single node. This means that a job is limited to the number of forks on a single Tower node. If we are executing a playbook across a large number of hosts then we are not making use of all of the available capacity in our cluster.</p>
</div>
<div class="paragraph">
<p>Job slicing is a new feature of Ansible Tower 3.4 and helps to address this issue. Job slicing allows us to distribute a job across multiple Tower nodes. It does this by splitting the inventory into slices. A workflow is then created for us with multiple instances of our job being run on each slice of our inventory. For example, if I have 30 nodes in my inventory and I decide to create 3 slices then a workflow is created with three jobs - each being executed on 10 nodes in parallel. Obviously, I need 3 Tower nodes in this example so that my three job slices can be executed across the three hosts.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_job_slices">Using job slices</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To demonstrate job slicing, I have a three node Tower cluster. My inventory contains 49 hosts for me to automate against. Here is the available capacity in forks reported by my hosts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/tower%20capacity.png" alt="tower capacity">
</div>
</div>
<div class="paragraph">
<p>My Tower hosts can each safely spawn up to 27 forks based on memory capacity. If we look at the following job template we can see that I have specified 30 forks - I run the risk of exhausting memory resources. Also note that I only have one job slice configured - this is the default when creating a new job template.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20template%201%20slice.png" alt="job template 1 slice">
</div>
</div>
<div class="paragraph">
<p>When I launch my job we can see that my playbook is being run on a single Tower node. As I have allocated more forks than I safely have capacity for I am over-allocated on capacity. My other two Tower nodes are just relaxing at this point with no work scheduled on them.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/1%20slice%20capacity.png" alt="1 slice capacity">
</div>
</div>
<div class="paragraph">
<p>Now I&#8217;ll re-configure my job template so that it utilises three slices.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20template%203%20slices.png" alt="job template 3 slices">
</div>
</div>
<div class="paragraph">
<p>This time when I launch my job template, a worklfow is automatically generated.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20slice%20workflow%20finished.png" alt="job slice workflow finished">
</div>
</div>
<div class="paragraph">
<p>If we look at one of the individual jobs within the workflow we can see that this is slice job 1 of 3. Also, note that this job is being run on a "slice" of my inventory - 16 hosts.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/job%20output.png" alt="job output">
</div>
</div>
<div class="paragraph">
<p>Finally, we can see that capacity is now being utilised across all three of my Tower nodes.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/tower%20slice/3%20slice%20capacity.png" alt="3 slice capacity">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Job slices provide an additional method for scaling Ansible automation with Ansible Tower. By default, jobs are distributed across Tower nodes in a cluster but each individual job is only ever run from a single Tower node. Job slicing allows us to split a single job so that it is run across multiple nodes to provide additional scale-out capacity.</p>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/04/17/Scaling-Ansible-Tower-Job-slicing.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/04/17/Scaling-Ansible-Tower-Job-slicing.html</guid><pubDate>Wed, 17 Apr 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[RHV Disaster Recovery - Part II]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In <a href="https://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html">part one</a> of this blog we looked at the different Disaster Recovery Solutions for RHV. In this part, we will look at the Active/Passive implementation in more detail, including how to set it up and run it. You&#8217;ll also find a recorded demo of the failover process.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A quick recap on what was discussed in part one of this blog. The Active/Passive architecture allows us to fail Virtual Machines over to a Disaster Recovery site in the event of a failure in the Primary site. The solution also provides the necessary orchestration to failback to the Primary site when appropriate. We leverage the power of Ansible to orchestrate the failover. The necessary Ansible roles are provided in a rpm called ovirt-ansible-disaster-recovery which is shipped as part of the RHV product.</p>
</div>
<div class="paragraph">
<p>From an underlying infrastructure perspective, we obviously need some hypervisors in our DR site to fail the workloads onto. The DR site also needs it&#8217;s own RHV Manager. We also rely on replicated storage between sites. The orchestration of the storage replication is not covered by the Ansible roles that are shipped with RHV. A storage administrator needs to setup the replication beforehand. They will also need to make the storage read/write in the secondary site at the point that DR is invoked.</p>
</div>
<div class="paragraph">
<p>Let&#8217;s take a high-level view of the DR process. In advance of the disaster, one must generate the mapping file which maps entities from the Primary site to the DR site. For example, workloads in "Cluster1" in the Primary site should be failed over to "Cluster2" in my DR site. This mapping file is used during the failover and failback process.</p>
</div>
<div class="paragraph">
<p>In the event of a disaster, the storage must be made read/write in the DR site. The process for this will vary depending on the nature of the disaster and the type of storage. The "failover" Ansible playbook can then be run. This will mount the storage domains at the DR site. Virtual Machines are then registered and started. Any VM&#8217;s marked as Highly Available are started first.</p>
</div>
<div class="paragraph">
<p>Once the Primary site is brought back online, it must be prepared. This is part of the "clean" Ansible playbook. Any replicated storage domains need to be removed from the RHV Manager. A storage administrator will need to ensure replication is now taking place from the DR site to the Primary site.</p>
</div>
<div class="paragraph">
<p>When ready, the failback playbook can be run. This stops the VM&#8217;s in the DR site and removes the storage domains from the RHV Manager. The playbook then pauses and waits for the administrator to confirm that replication has been stopped and the storage is now read/write in the Primary site. Once confirmed, the playbook will import the storage domains at the Primary site, register the VM&#8217;s and start them up. The final step is to ensure the storage replication is switched so that the Primary site is once again replicating to the DR site.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo_environment">Demo Environment</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I am using nested virtualisation for my lab. For the storage replication, I am using GlusterFS and geo-replication. This is asynchronous replication which is <strong>not</strong> recommended but it serves the purpose for this demo. Synchronous replication is recommended for replication between sites.</p>
</div>
<div class="paragraph">
<p>Each "site" has a RHV Manager. The RHV Manager is configured with a single cluster containing a singe RHV Host. I&#8217;ve got two small cirros VM&#8217;s which I will failover in this demo. One is marked as Highly Available to demonstrate the fact that HA VM&#8217;s are started before standard VM&#8217;s.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%20Lab.png" alt="RHV DR Active Passive Lab">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_setup">Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p><em>All of the files mentioned in this setup section are available in my github repo <a href="https://github.com/pharriso/rhv-dr">here</a></em>.</p>
</div>
<div class="paragraph">
<p><strong>Don&#8217;t worry. All of the below playbooks are fully documented in the Red Hat documentation for RHV DR.</strong></p>
</div>
<div class="sect2">
<h3 id="_generate_mapping">Generate Mapping</h3>
<div class="paragraph">
<p>First create a variable file containing the passwords for the RHV manager. I&#8217;m creating a file called passwords.yml here with these contents:</p>
</div>
<div class="listingblock">
<div class="title">passwords.yml</div>
<div class="content">
<pre class="highlight"><code>---
dr_sites_primary_password: Redhat123
dr_sites_secondary_password: Redhat123</code></pre>
</div>
</div>
<div class="paragraph">
<p>Encrypt the file with ansible-vault to avoid leaving passwords in plaintext.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-vault encrypt password.yml</pre>
</div>
</div>
<div class="paragraph">
<p>Now we can create the playbook that will generate the mapping file for us.</p>
</div>
<div class="listingblock">
<div class="title">generate_mappings.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Generate mapping
  hosts: localhost
  connection: local

  vars:
    site: https://rhvm-primary.example.com/ovirt-engine/api
    username: admin@internal
    password: "{{ dr_sites_primary_password }}"
    ca: /root/DR/primary_ca.pem
    var_file: disaster_recovery_vars.yml

  vars_files:
    - passwords.yml

  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now run the playbook to generate the mapping variable file. This will log into the RHV Manager at the Primary site and retrieve details on components such as storage, networking and clusters.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook generate_mappings.yml --tags generate_mapping --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>A file called disaster_recovery_vars.yml is generated. This needs to be edited to allow you to map components from the Primary site to the Disaster site. For example, the login details for the Disaster site:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>dr_sites_primary_url: https://rhvm-primary.example.com/ovirt-engine/api
dr_sites_primary_username: admin@internal
dr_sites_primary_ca_file: /root/DR/primary_ca.pem

# Please fill in the following properties for the secondary site:
dr_sites_secondary_url:  https://rhvm-secondary.example.com/ovirt-engine/api
dr_sites_secondary_username:  admin@internal
dr_sites_secondary_ca_file: /etc/pki/ovirt-engine/ca.pem</pre>
</div>
</div>
<div class="paragraph">
<p>And also the cluster mappings:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># Mapping for cluster
dr_cluster_mappings:
- primary_name: Primary
  # Fill the correlated cluster name in the secondary site for cluster 'Primary'
  secondary_name: Disaster</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_failover_failback_playbooks">Create Failover/Failback Playbooks</h3>
<div class="paragraph">
<p>Now we can create the playbook that we will use to initiate the failover from our Primary site to our DR site:</p>
</div>
<div class="listingblock">
<div class="title">failover.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failover RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: secondary
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>And the failback playbook to allow us to failback to our Primary site once it has been restored (The same playbook but with the source and target reversed):</p>
</div>
<div class="listingblock">
<div class="title">failback.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: Failback RHV
  hosts: localhost
  connection: local
  vars:
    dr_target_host: primary
    dr_source_map: secondary
  vars_files:
    - disaster_recovery_vars.yml
    - passwords.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, the cleanup playbook. This is used to clean the Primary site ready for failback:</p>
</div>
<div class="listingblock">
<div class="title">clean_primary.yml</div>
<div class="content">
<pre class="highlight"><code>---
- name: clean RHV
  hosts: localhost
  connection: local
  vars:
    dr_source_map: primary
  vars_files:
    - disaster_recovery_vars.yml
  roles:
    - oVirt.disaster-recovery</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_failover">Failover</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To failover we need to ensure that the storage replication is stopped and is made read/write in the DR site. Once this is confirmed, we can run the Ansible playbook to failover.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failover.yaml --tags fail_over --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cleanup_and_failback">Cleanup and Failback</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the Primary site is brought back online we can begin the failback process. Storage now needs to be replicated from DR site back to Primary site. The primary site also needs to be cleaned to ensure storage domains are not imported.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook clean_primary.yml --tags clean_engine --ask-vault-pass</pre>
</div>
</div>
<div class="paragraph">
<p>When ready, initiate the failback. The playbook will pause and wait for you to confirm that the storage replication has been stopped and that storage domains are now read/write in the Primary site.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>ansible-playbook failback.yml --tags fail_back --ask-vault-pass</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo">Demo</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The following video demonstrates the failover process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/OC66G7_y8Vo?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/16/RHV-Disaster-Recovery-Part-II.html</guid><pubDate>Wed, 16 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Infrastructure Migration Solution]]></title><description><![CDATA[<div class="paragraph">
<p>While VMware is undoubtedly a great virtualisation offering (I spent a number of users working with VMware tech) some customers are looking beyond traditional virtualisation. The growing adoption of containerisation is a great example of this and using RHV as a stepping stone to Container Native Virtualisation (CNV) is a good example use case.</p>
</div>
<div class="paragraph">
<p>Other customers simply want to save costs and are looking at alternative virtualisation offerings. Whatever the reason for moving from VMware, we need a method for migrating workloads to RHV.</p>
</div>
<div class="paragraph">
<p>This is what our Infrastructure Migration Solution (IMS) provides. A simple method for migrating workloads from VMware to RHV. We use a tool called CloudForms to drive the migration process (ManageIQ is the upstream project for CloudForms).</p>
</div>
<div class="paragraph">
<p>CloudForms is a manager of managers and can interact with various endpoints which are known as providers. A provider could be VMware vSphere, Red Hat Virtualisation, AWS, OpenStack etc. CloudForms is able to ingest information from these providers such as configuration details and performance metrics as well as orchestrate these providers.</p>
</div>
<div class="paragraph">
<p>Using the new CloudForms "migration" tooling we can create mappings between VMware infrastructure and Red Hat Virtualisation. These include clusters, storage and networking. We can then detect the VM&#8217;s currently running in VMware and using virt-v2v under the covers we can automate the migration of those VM&#8217;s across to RHV.</p>
</div>
<div class="paragraph">
<p>This short video shows a demo of the process.</p>
</div>
<div class="videoblock">
<div class="content">
<iframe src="https://www.youtube.com/embed/NdjGuJaDSOU?rel=0" frameborder="0" allowfullscreen></iframe>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/09/Infrastructure-Migration-Solution.html</guid><pubDate>Wed, 09 Jan 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[RHV Disaster Recovery - Part I]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of Red Hat Virtualisation (RHV) 4.1 back in 2017 we have seen the inclusion of Disaster Recovery (DR) solutions. The aim of these solutions is to allow customers to deploy their RHV infrastructure to span multiple sites and allow failover of virtual machines in the event of a disaster. This post will take a look at these Disaster Recovery solutions.</p>
</div>
<div class="paragraph">
<p>It is worth noting that these DR solutions are part of the core RHV product and are NOT part of a separate subscription offering. There is no per Virtual Machine (VM) cost or similar charge for protecting your workloads.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview">Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>RHV 4.1 was released back in 2017 and we introduced an Active/Active implementation. With the release of RHV 4.2 in May 2018 we introduced additional disaster recovery capabilities. We can now deploy an Active/Passive architecture spanning two sites. Let&#8217;s take a look at the different implementation options.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_active">Active/Active</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Active architecture can span two sites. With this implementation we deploy a single RHV cluster with RHV hosts spanning both sites in a stretch cluster configuration. A single RHV Manager is responsible for managing both sites. The RHV Manager itself could be self hosted or standalone (self-hosted simply means the RHV Manager is a VM running on the hypervisors it is managing).</p>
</div>
<div class="paragraph">
<p>As both sites are active at the same time and VM&#8217;s can effectively run at either site at any given time we require synchronously replicated storage that is writeable at both sites at the same time.</p>
</div>
<div class="paragraph">
<p>In addition to the storage replication we also need a stretched network between sites. As we are deploying a single cluster across both sites we need all of the RHV hosts to be in the same Layer 2 network segment. Also VM networks need to be stretched between sites so that that VM&#8217;s can migrate or failover to the secondary site and maintain network connectivity.</p>
</div>
<div class="paragraph">
<p>VM to RHV Host affinity can be used to ensure VM&#8217;s are running in the Primary datacenter where possible and only failed over as part of a disaster. In the event of a disaster in the Primary site, any VM&#8217;s marked as "highly available" will automatically be restarted in the Disaster site without any administrator intervention.</p>
</div>
<div class="paragraph">
<p>The nice thing about the Active/Active setup is that we just rely on native RHV HA to fail VM&#8217;s between sites. It does rely on a storage array that can provide write access to both sites with replication. An example of this would be something like EMC VPLEX.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Active configuration looks like. This diagram depicts a the RHV manager as a self-hosted engine which is failed over along with the VM workloads.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%201.png" alt="RHV DR Active Active 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a failure at the Primary site, any VM&#8217;s marked as highly available will automatically restart on RHV hosts in the DR site. Also, as the RHV Manager is self-hosted it is also restarted at the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Active%202.png" alt="RHV DR Active Active 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_active_passive">Active/Passive</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The Active/Passive configuration can also be used across two sites. As the name suggests, only one site is active at any given time. In the event of a disaster, VM&#8217;s are failed over from the Primary site to the Disaster site. Once the Primary site is recovered we can then failback. Both failover and failback occur in an automated fashion using Ansible.</p>
</div>
<div class="paragraph">
<p>Unlike the Active/Active architecture, each site must maintain its own RHV Manager which manages the RHV hosts, storage and networks for that site.</p>
</div>
<div class="paragraph">
<p>From the storage perspective we require replicated storage between sites. However, the storage is only ever attached to one RHV site at a time so it does not need to be writeable at both sites simultaneously. The same VM networks need to be available in both sites so that VM&#8217;s that are failed over can be re-attached to the network.</p>
</div>
<div class="paragraph">
<p>Here is what an Active/Passive configuration looks like.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%201.png" alt="RHV DR Active Passive 1">
</div>
</div>
<div class="paragraph">
<p>In the event of a disaster at the Primary site then we need to ensure that any replication of storage is stopped and that the storage is changed to read/write at the DR site and readonly at Primary site. An administrator can then initiate a failover of VM&#8217;s to the DR site.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://cloudautomation.pharriso.co.uk/images/RHV%20DR%20Active%20Passive%202.png" alt="RHV DR Active Passive 2">
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_summary">Summary</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Since the release of RHV 4.2 we now have two options for providing Disaster Recovery for our VM&#8217;s.</p>
</div>
<div class="paragraph">
<p>The Active/Active configuration allows us to easily fail workloads between sites using VM HA. However, it does rely on storage which is read/write at both sites at the same time as well as being synchronously replicated.</p>
</div>
<div class="paragraph">
<p>The Active/Passive solution only requires replicated storage in read/write at one site at a time. However, failover requires manual intervention to switch storage replication and also initiate the VM failover via Ansible.</p>
</div>
<div class="paragraph">
<p>In the next part of this post we will take a look at how to setup RHV DR in an Active/Passive configuration and how to perform failover and failback.</p>
</div>
</div>
</div>]]></description><link>http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</link><guid isPermaLink="true">http://cloudautomation.pharriso.co.uk/2019/01/08/RHV-Disaster-Recovery-Part-I.html</guid><pubDate>Tue, 08 Jan 2019 00:00:00 GMT</pubDate></item></channel></rss>